{
  "mode": "both",
  "description": "Both models tested (Baseline first, then Progressive)",
  "experiments": [
    {
      "mode": "both",
      "timestamp": "2026-01-09T18:46:47.157756",
      "baseline": {
        "model_type": "baseline",
        "model_path": "/acpl-ssd20/Llama-2-7b",
        "timestamp": "2026-01-09T18:47:16.693033",
        "cold_start_time": 29.021778106689453,
        "ttft": 0.031122207641601562,
        "output_throughput": 207.4612373864153,
        "gpu_memory_gb": 27.77156400680542,
        "total_layers": 32,
        "active_layers": 32,
        "network_metrics": {
          "error": true,
          "error_message": "Interface 'bond0.88' not found. Available interfaces: ['lo', 'eth0']",
          "interface": "bond0.88"
        },
        "vllm_logs": {
          "weight_loading_time": 12.66,
          "model_loading_gb": 12.5523,
          "model_loading_time": 12.769488,
          "memory_profiling_time": 0.65,
          "total_gpu_memory_gb": 31.36,
          "gpu_memory_utilization": 0.9,
          "model_weights_gb": 12.55,
          "kv_cache_gb": 15.21,
          "cuda_blocks": 1946,
          "cpu_blocks": 512,
          "max_concurrency": 7.6,
          "graph_capturing_time": 8.0,
          "graph_capturing_memory_gb": 0.29,
          "init_engine_time": 10.4,
          "raw_logs": [
            "This model supports multiple tasks: {'generate', 'reward', 'classify', 'score', 'embed'}. Defaulting to 'generate'.",
            "Initializing a V0 LLM engine (v0.7.4.dev254+ged6ea0657.d20260102) with config: model='/acpl-ssd20/Llama-2-7b', speculative_config=None, tokenizer='/acpl-ssd20/Llama-2-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/acpl-ssd20/Llama-2-7b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, ",
            "Using Flash Attention backend.",
            "rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0",
            "Starting to load model /acpl-ssd20/Llama-2-7b...",
            "Loading weights took 12.66 seconds",
            "Model loading took 12.5523 GB and 12.769488 seconds",
            "Memory profiling takes 0.65 seconds\nthe current vLLM instance can use total_gpu_memory (31.36GiB) x gpu_memory_utilization (0.90) = 28.22GiB\nmodel weights take 12.55GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.35GiB; the rest of the memory reserved for KV Cache is 15.21GiB.",
            "# cuda blocks: 1946, # CPU blocks: 512",
            "Maximum concurrency for 4096 tokens per request: 7.60x",
            "Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.",
            "Graph capturing finished in 8 secs, took 0.29 GiB",
            "init engine (profile, create kv cache, warmup model) took 10.40 seconds"
          ]
        }
      },
      "progressive": {
        "model_type": "progressive",
        "model_path": "/acpl-ssd20/1218/A",
        "timestamp": "2026-01-09T18:47:37.409885",
        "cold_start_time": 18.165217399597168,
        "ttft": 0.021860837936401367,
        "output_throughput": 206.04714865956248,
        "gpu_memory_gb": 27.826388835906982,
        "total_layers": 32,
        "active_layers": 24,
        "network_metrics": {
          "error": true,
          "error_message": "Interface 'bond0.88' not found. Available interfaces: ['lo', 'eth0']",
          "interface": "bond0.88"
        },
        "vllm_logs": {
          "weight_loading_time": 9.24,
          "model_loading_gb": 12.5514,
          "model_loading_time": 9.278255,
          "memory_profiling_time": 0.5,
          "total_gpu_memory_gb": 31.36,
          "gpu_memory_utilization": 0.9,
          "model_weights_gb": 12.55,
          "kv_cache_gb": 15.24,
          "cuda_blocks": 1950,
          "cpu_blocks": 512,
          "max_concurrency": 7.62,
          "graph_capturing_time": 7.0,
          "graph_capturing_memory_gb": 0.07,
          "init_engine_time": 8.4,
          "raw_logs": [
            "This model supports multiple tasks: {'generate', 'reward', 'classify', 'score', 'embed'}. Defaulting to 'generate'.",
            "Initializing a V0 LLM engine (v0.7.4.dev254+ged6ea0657.d20260102) with config: model='/acpl-ssd20/1218/A', speculative_config=None, tokenizer='/acpl-ssd20/1218/A', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/acpl-ssd20/1218/A, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, ",
            "Starting to load model /acpl-ssd20/1218/A...",
            "Loading weights took 9.24 seconds",
            "Model loading took 12.5514 GB and 9.278255 seconds",
            "Memory profiling takes 0.50 seconds\nthe current vLLM instance can use total_gpu_memory (31.36GiB) x gpu_memory_utilization (0.90) = 28.22GiB\nmodel weights take 12.55GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.43GiB; the rest of the memory reserved for KV Cache is 15.24GiB.",
            "# cuda blocks: 1950, # CPU blocks: 512",
            "Maximum concurrency for 4096 tokens per request: 7.62x",
            "Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.",
            "Graph capturing finished in 7 secs, took 0.07 GiB",
            "init engine (profile, create kv cache, warmup model) took 8.40 seconds"
          ]
        },
        "activation_progress": "75%",
        "layers_pruned": 8
      },
      "experiment_id": "2a1bbbd1-e4e7-4c7f-aa5d-81020dcaa542",
      "hostname": "e3a8e045298d"
    },
    {
      "mode": "both",
      "timestamp": "2026-01-09T19:01:50.949799",
      "baseline": {
        "model_type": "baseline",
        "model_path": "/acpl-ssd20/Llama-2-7b",
        "timestamp": "2026-01-09T19:02:23.149439",
        "cold_start_time": 31.68606948852539,
        "ttft": 0.031057357788085938,
        "output_throughput": 207.39609684507835,
        "gpu_memory_gb": 27.77156400680542,
        "total_layers": 32,
        "active_layers": 32,
        "network_metrics": {
          "error": true,
          "error_message": "Interface 'bond0.88' not found. Available interfaces: ['lo', 'eth0']",
          "interface": "bond0.88"
        },
        "vllm_logs": {
          "weight_loading_time": 15.5,
          "model_loading_gb": 12.5523,
          "model_loading_time": 15.60861,
          "memory_profiling_time": 0.65,
          "total_gpu_memory_gb": 31.36,
          "gpu_memory_utilization": 0.9,
          "model_weights_gb": 12.55,
          "kv_cache_gb": 15.21,
          "cuda_blocks": 1946,
          "cpu_blocks": 512,
          "max_concurrency": 7.6,
          "graph_capturing_time": 8.0,
          "graph_capturing_memory_gb": 0.29,
          "init_engine_time": 10.19,
          "raw_logs": [
            "This model supports multiple tasks: {'classify', 'reward', 'generate', 'score', 'embed'}. Defaulting to 'generate'.",
            "Initializing a V0 LLM engine (v0.7.4.dev254+ged6ea0657.d20260102) with config: model='/acpl-ssd20/Llama-2-7b', speculative_config=None, tokenizer='/acpl-ssd20/Llama-2-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/acpl-ssd20/Llama-2-7b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, ",
            "Using Flash Attention backend.",
            "rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0",
            "Starting to load model /acpl-ssd20/Llama-2-7b...",
            "Loading weights took 15.50 seconds",
            "Model loading took 12.5523 GB and 15.608610 seconds",
            "Memory profiling takes 0.65 seconds\nthe current vLLM instance can use total_gpu_memory (31.36GiB) x gpu_memory_utilization (0.90) = 28.22GiB\nmodel weights take 12.55GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.35GiB; the rest of the memory reserved for KV Cache is 15.21GiB.",
            "# cuda blocks: 1946, # CPU blocks: 512",
            "Maximum concurrency for 4096 tokens per request: 7.60x",
            "Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.",
            "Graph capturing finished in 8 secs, took 0.29 GiB",
            "init engine (profile, create kv cache, warmup model) took 10.19 seconds"
          ]
        }
      },
      "progressive": {
        "model_type": "progressive",
        "model_path": "/acpl-ssd20/1218/A",
        "timestamp": "2026-01-09T19:02:43.514774",
        "cold_start_time": 17.812777280807495,
        "ttft": 0.021651506423950195,
        "output_throughput": 205.87652917526265,
        "gpu_memory_gb": 27.826388835906982,
        "total_layers": 32,
        "active_layers": 24,
        "network_metrics": {
          "error": true,
          "error_message": "Interface 'bond0.88' not found. Available interfaces: ['lo', 'eth0']",
          "interface": "bond0.88"
        },
        "vllm_logs": {
          "weight_loading_time": 8.86,
          "model_loading_gb": 12.5514,
          "model_loading_time": 8.899524,
          "memory_profiling_time": 0.49,
          "total_gpu_memory_gb": 31.36,
          "gpu_memory_utilization": 0.9,
          "model_weights_gb": 12.55,
          "kv_cache_gb": 15.24,
          "cuda_blocks": 1950,
          "cpu_blocks": 512,
          "max_concurrency": 7.62,
          "graph_capturing_time": 8.0,
          "graph_capturing_memory_gb": 0.07,
          "init_engine_time": 8.43,
          "raw_logs": [
            "This model supports multiple tasks: {'classify', 'reward', 'generate', 'score', 'embed'}. Defaulting to 'generate'.",
            "Initializing a V0 LLM engine (v0.7.4.dev254+ged6ea0657.d20260102) with config: model='/acpl-ssd20/1218/A', speculative_config=None, tokenizer='/acpl-ssd20/1218/A', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/acpl-ssd20/1218/A, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, ",
            "Starting to load model /acpl-ssd20/1218/A...",
            "Loading weights took 8.86 seconds",
            "Model loading took 12.5514 GB and 8.899524 seconds",
            "Memory profiling takes 0.49 seconds\nthe current vLLM instance can use total_gpu_memory (31.36GiB) x gpu_memory_utilization (0.90) = 28.22GiB\nmodel weights take 12.55GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.43GiB; the rest of the memory reserved for KV Cache is 15.24GiB.",
            "# cuda blocks: 1950, # CPU blocks: 512",
            "Maximum concurrency for 4096 tokens per request: 7.62x",
            "Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.",
            "Graph capturing finished in 8 secs, took 0.07 GiB",
            "init engine (profile, create kv cache, warmup model) took 8.43 seconds"
          ]
        },
        "activation_progress": "75%",
        "layers_pruned": 8
      },
      "experiment_id": "ef66a6da-3f64-4f82-9da3-8fc81b39ba73",
      "hostname": "e3a8e045298d"
    },
    {
      "mode": "both",
      "timestamp": "2026-01-09T19:11:21.024066",
      "baseline": {
        "model_type": "baseline",
        "model_path": "/acpl-ssd20/Llama-2-7b",
        "timestamp": "2026-01-09T19:11:50.694767",
        "cold_start_time": 29.157290935516357,
        "ttft": 0.03159952163696289,
        "output_throughput": 207.69166015510876,
        "gpu_memory_gb": 27.77156400680542,
        "total_layers": 32,
        "active_layers": 32,
        "network_metrics": {
          "error": true,
          "error_message": "Interface 'bond0.88' not found. Available interfaces: ['lo', 'eth0']",
          "interface": "bond0.88"
        },
        "vllm_logs": {
          "weight_loading_time": 12.89,
          "model_loading_gb": 12.5523,
          "model_loading_time": 13.002513,
          "memory_profiling_time": 0.65,
          "total_gpu_memory_gb": 31.36,
          "gpu_memory_utilization": 0.9,
          "model_weights_gb": 12.55,
          "kv_cache_gb": 15.21,
          "cuda_blocks": 1946,
          "cpu_blocks": 512,
          "max_concurrency": 7.6,
          "graph_capturing_time": 8.0,
          "graph_capturing_memory_gb": 0.29,
          "init_engine_time": 10.28,
          "raw_logs": [
            "This model supports multiple tasks: {'score', 'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.",
            "Initializing a V0 LLM engine (v0.7.4.dev254+ged6ea0657.d20260102) with config: model='/acpl-ssd20/Llama-2-7b', speculative_config=None, tokenizer='/acpl-ssd20/Llama-2-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/acpl-ssd20/Llama-2-7b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, ",
            "Using Flash Attention backend.",
            "rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0",
            "Starting to load model /acpl-ssd20/Llama-2-7b...",
            "Loading weights took 12.89 seconds",
            "Model loading took 12.5523 GB and 13.002513 seconds",
            "Memory profiling takes 0.65 seconds\nthe current vLLM instance can use total_gpu_memory (31.36GiB) x gpu_memory_utilization (0.90) = 28.22GiB\nmodel weights take 12.55GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.35GiB; the rest of the memory reserved for KV Cache is 15.21GiB.",
            "# cuda blocks: 1946, # CPU blocks: 512",
            "Maximum concurrency for 4096 tokens per request: 7.60x",
            "Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.",
            "Graph capturing finished in 8 secs, took 0.29 GiB",
            "init engine (profile, create kv cache, warmup model) took 10.28 seconds"
          ]
        }
      },
      "progressive": {
        "model_type": "progressive",
        "model_path": "/acpl-ssd20/1218/A",
        "timestamp": "2026-01-09T19:12:11.559490",
        "cold_start_time": 18.313257932662964,
        "ttft": 0.02182173728942871,
        "output_throughput": 206.13920022411222,
        "gpu_memory_gb": 27.826388835906982,
        "total_layers": 32,
        "active_layers": 24,
        "network_metrics": {
          "error": true,
          "error_message": "Interface 'bond0.88' not found. Available interfaces: ['lo', 'eth0']",
          "interface": "bond0.88"
        },
        "vllm_logs": {
          "weight_loading_time": 9.29,
          "model_loading_gb": 12.5514,
          "model_loading_time": 9.3348,
          "memory_profiling_time": 0.5,
          "total_gpu_memory_gb": 31.36,
          "gpu_memory_utilization": 0.9,
          "model_weights_gb": 12.55,
          "kv_cache_gb": 15.24,
          "cuda_blocks": 1950,
          "cpu_blocks": 512,
          "max_concurrency": 7.62,
          "graph_capturing_time": 8.0,
          "graph_capturing_memory_gb": 0.07,
          "init_engine_time": 8.48,
          "raw_logs": [
            "This model supports multiple tasks: {'score', 'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.",
            "Initializing a V0 LLM engine (v0.7.4.dev254+ged6ea0657.d20260102) with config: model='/acpl-ssd20/1218/A', speculative_config=None, tokenizer='/acpl-ssd20/1218/A', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/acpl-ssd20/1218/A, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, ",
            "Starting to load model /acpl-ssd20/1218/A...",
            "Loading weights took 9.29 seconds",
            "Model loading took 12.5514 GB and 9.334800 seconds",
            "Memory profiling takes 0.50 seconds\nthe current vLLM instance can use total_gpu_memory (31.36GiB) x gpu_memory_utilization (0.90) = 28.22GiB\nmodel weights take 12.55GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.43GiB; the rest of the memory reserved for KV Cache is 15.24GiB.",
            "# cuda blocks: 1950, # CPU blocks: 512",
            "Maximum concurrency for 4096 tokens per request: 7.62x",
            "Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.",
            "Graph capturing finished in 8 secs, took 0.07 GiB",
            "init engine (profile, create kv cache, warmup model) took 8.48 seconds"
          ]
        },
        "activation_progress": "75%",
        "layers_pruned": 8
      },
      "experiment_id": "87e0e7e3-bf92-4ea6-a90a-e1408ac1a012",
      "hostname": "e3a8e045298d"
    },
    {
      "mode": "both",
      "timestamp": "2026-01-19T17:42:46.006768",
      "baseline": {
        "model_type": "baseline",
        "model_path": "/acpl-ssd20/Llama-2-7b",
        "timestamp": "2026-01-19T17:43:20.848696",
        "cold_start_time": 34.3107807636261,
        "ttft": 0.04041862487792969,
        "output_throughput": 203.89848594228155,
        "gpu_memory_gb": 27.77156400680542,
        "total_layers": 32,
        "active_layers": 32,
        "network_metrics": {
          "error": true,
          "error_message": "Interface 'bond0.88' not found. Available interfaces: ['lo', 'eth0']",
          "interface": "bond0.88"
        },
        "vllm_logs": {
          "weight_loading_time": 16.63,
          "model_loading_gb": 12.5523,
          "model_loading_time": 16.776698,
          "memory_profiling_time": 0.7,
          "total_gpu_memory_gb": 31.36,
          "gpu_memory_utilization": 0.9,
          "model_weights_gb": 12.55,
          "kv_cache_gb": 15.21,
          "cuda_blocks": 1946,
          "cpu_blocks": 512,
          "max_concurrency": 7.6,
          "graph_capturing_time": 8.0,
          "graph_capturing_memory_gb": 0.29,
          "init_engine_time": 10.79,
          "raw_logs": [
            "This model supports multiple tasks: {'classify', 'score', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.",
            "Initializing a V0 LLM engine (v0.7.4.dev254+ged6ea0657.d20260102) with config: model='/acpl-ssd20/Llama-2-7b', speculative_config=None, tokenizer='/acpl-ssd20/Llama-2-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/acpl-ssd20/Llama-2-7b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, ",
            "Using Flash Attention backend.",
            "rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0",
            "Starting to load model /acpl-ssd20/Llama-2-7b...",
            "Loading weights took 16.63 seconds",
            "Model loading took 12.5523 GB and 16.776698 seconds",
            "Memory profiling takes 0.70 seconds\nthe current vLLM instance can use total_gpu_memory (31.36GiB) x gpu_memory_utilization (0.90) = 28.22GiB\nmodel weights take 12.55GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.35GiB; the rest of the memory reserved for KV Cache is 15.21GiB.",
            "# cuda blocks: 1946, # CPU blocks: 512",
            "Maximum concurrency for 4096 tokens per request: 7.60x",
            "Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.",
            "Graph capturing finished in 8 secs, took 0.29 GiB",
            "init engine (profile, create kv cache, warmup model) took 10.79 seconds"
          ]
        }
      },
      "progressive": {
        "model_type": "progressive",
        "model_path": "/acpl-ssd20/1218/A",
        "timestamp": "2026-01-19T17:43:46.793701",
        "cold_start_time": 23.38197684288025,
        "ttft": 0.0219876766204834,
        "output_throughput": 202.74120660461128,
        "gpu_memory_gb": 27.826388835906982,
        "total_layers": 32,
        "active_layers": 24,
        "network_metrics": {
          "error": true,
          "error_message": "Interface 'bond0.88' not found. Available interfaces: ['lo', 'eth0']",
          "interface": "bond0.88"
        },
        "vllm_logs": {
          "weight_loading_time": 13.84,
          "model_loading_gb": 12.5514,
          "model_loading_time": 13.881106,
          "memory_profiling_time": 0.51,
          "total_gpu_memory_gb": 31.36,
          "gpu_memory_utilization": 0.9,
          "model_weights_gb": 12.55,
          "kv_cache_gb": 15.24,
          "cuda_blocks": 1950,
          "cpu_blocks": 512,
          "max_concurrency": 7.62,
          "graph_capturing_time": 8.0,
          "graph_capturing_memory_gb": 0.07,
          "init_engine_time": 8.97,
          "raw_logs": [
            "This model supports multiple tasks: {'classify', 'score', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.",
            "Initializing a V0 LLM engine (v0.7.4.dev254+ged6ea0657.d20260102) with config: model='/acpl-ssd20/1218/A', speculative_config=None, tokenizer='/acpl-ssd20/1218/A', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/acpl-ssd20/1218/A, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, ",
            "Starting to load model /acpl-ssd20/1218/A...",
            "Loading weights took 13.84 seconds",
            "Model loading took 12.5514 GB and 13.881106 seconds",
            "Memory profiling takes 0.51 seconds\nthe current vLLM instance can use total_gpu_memory (31.36GiB) x gpu_memory_utilization (0.90) = 28.22GiB\nmodel weights take 12.55GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.43GiB; the rest of the memory reserved for KV Cache is 15.24GiB.",
            "# cuda blocks: 1950, # CPU blocks: 512",
            "Maximum concurrency for 4096 tokens per request: 7.62x",
            "Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.",
            "Graph capturing finished in 8 secs, took 0.07 GiB",
            "init engine (profile, create kv cache, warmup model) took 8.97 seconds"
          ]
        },
        "activation_progress": "75%",
        "layers_pruned": 8
      },
      "experiment_id": "4d9768a3-0099-40db-adf5-f9d26a336290",
      "hostname": "e3a8e045298d"
    }
  ]
}