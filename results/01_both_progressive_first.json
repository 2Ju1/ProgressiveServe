{
  "mode": "reversed",
  "description": "Both models tested (Progressive first, then Baseline)",
  "experiments": [
    {
      "mode": "reversed",
      "timestamp": "2026-01-09T18:49:17.326534",
      "progressive": {
        "model_type": "progressive",
        "model_path": "/acpl-ssd20/1218/A",
        "timestamp": "2026-01-09T18:49:38.154477",
        "cold_start_time": 20.310659408569336,
        "ttft": 0.031759023666381836,
        "output_throughput": 206.11721784529257,
        "gpu_memory_gb": 27.693454265594482,
        "total_layers": 32,
        "active_layers": 24,
        "network_metrics": {
          "error": true,
          "error_message": "Interface 'bond0.88' not found. Available interfaces: ['lo', 'eth0']",
          "interface": "bond0.88"
        },
        "vllm_logs": {
          "weight_loading_time": 9.06,
          "model_loading_gb": 12.5523,
          "model_loading_time": 9.177245,
          "memory_profiling_time": 0.7,
          "total_gpu_memory_gb": 31.36,
          "gpu_memory_utilization": 0.9,
          "model_weights_gb": 12.55,
          "kv_cache_gb": 15.12,
          "cuda_blocks": 1935,
          "cpu_blocks": 512,
          "max_concurrency": 7.56,
          "graph_capturing_time": 8.0,
          "graph_capturing_memory_gb": 0.29,
          "init_engine_time": 10.36,
          "raw_logs": [
            "This model supports multiple tasks: {'classify', 'generate', 'score', 'embed', 'reward'}. Defaulting to 'generate'.",
            "Initializing a V0 LLM engine (v0.7.4.dev254+ged6ea0657.d20260102) with config: model='/acpl-ssd20/1218/A', speculative_config=None, tokenizer='/acpl-ssd20/1218/A', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/acpl-ssd20/1218/A, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, ",
            "Using Flash Attention backend.",
            "rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0",
            "Starting to load model /acpl-ssd20/1218/A...",
            "Loading weights took 9.06 seconds",
            "Model loading took 12.5523 GB and 9.177245 seconds",
            "Memory profiling takes 0.70 seconds\nthe current vLLM instance can use total_gpu_memory (31.36GiB) x gpu_memory_utilization (0.90) = 28.22GiB\nmodel weights take 12.55GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.44GiB; the rest of the memory reserved for KV Cache is 15.12GiB.",
            "# cuda blocks: 1935, # CPU blocks: 512",
            "Maximum concurrency for 4096 tokens per request: 7.56x",
            "Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.",
            "Graph capturing finished in 8 secs, took 0.29 GiB",
            "init engine (profile, create kv cache, warmup model) took 10.36 seconds"
          ]
        },
        "activation_progress": "75%",
        "layers_pruned": 8
      },
      "baseline": {
        "model_type": "baseline",
        "model_path": "/acpl-ssd20/Llama-2-7b",
        "timestamp": "2026-01-09T18:50:09.585305",
        "cold_start_time": 28.88205051422119,
        "ttft": 0.021485567092895508,
        "output_throughput": 207.73928776442864,
        "gpu_memory_gb": 27.89668607711792,
        "total_layers": 32,
        "active_layers": 32,
        "network_metrics": {
          "error": true,
          "error_message": "Interface 'bond0.88' not found. Available interfaces: ['lo', 'eth0']",
          "interface": "bond0.88"
        },
        "vllm_logs": {
          "weight_loading_time": 15.08,
          "model_loading_gb": 12.5513,
          "model_loading_time": 15.103162,
          "memory_profiling_time": 0.43,
          "total_gpu_memory_gb": 31.36,
          "gpu_memory_utilization": 0.9,
          "model_weights_gb": 12.55,
          "kv_cache_gb": 15.32,
          "cuda_blocks": 1961,
          "cpu_blocks": 512,
          "max_concurrency": 7.66,
          "graph_capturing_time": 8.0,
          "graph_capturing_memory_gb": 0.06,
          "init_engine_time": 8.43,
          "raw_logs": [
            "This model supports multiple tasks: {'classify', 'generate', 'score', 'embed', 'reward'}. Defaulting to 'generate'.",
            "Initializing a V0 LLM engine (v0.7.4.dev254+ged6ea0657.d20260102) with config: model='/acpl-ssd20/Llama-2-7b', speculative_config=None, tokenizer='/acpl-ssd20/Llama-2-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/acpl-ssd20/Llama-2-7b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, ",
            "Starting to load model /acpl-ssd20/Llama-2-7b...",
            "Loading weights took 15.08 seconds",
            "Model loading took 12.5513 GB and 15.103162 seconds",
            "Memory profiling takes 0.43 seconds\nthe current vLLM instance can use total_gpu_memory (31.36GiB) x gpu_memory_utilization (0.90) = 28.22GiB\nmodel weights take 12.55GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.35GiB; the rest of the memory reserved for KV Cache is 15.32GiB.",
            "# cuda blocks: 1961, # CPU blocks: 512",
            "Maximum concurrency for 4096 tokens per request: 7.66x",
            "Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.",
            "Graph capturing finished in 8 secs, took 0.06 GiB",
            "init engine (profile, create kv cache, warmup model) took 8.43 seconds"
          ]
        }
      },
      "experiment_id": "a7ea6b31-e0be-4d98-afbf-7487740df74e",
      "hostname": "e3a8e045298d"
    },
    {
      "mode": "reversed",
      "timestamp": "2026-01-09T19:03:02.180373",
      "progressive": {
        "model_type": "progressive",
        "model_path": "/acpl-ssd20/1218/A",
        "timestamp": "2026-01-09T19:03:22.663637",
        "cold_start_time": 19.965808629989624,
        "ttft": 0.03180050849914551,
        "output_throughput": 206.04927433565715,
        "gpu_memory_gb": 27.693454265594482,
        "total_layers": 32,
        "active_layers": 24,
        "network_metrics": {
          "error": true,
          "error_message": "Interface 'bond0.88' not found. Available interfaces: ['lo', 'eth0']",
          "interface": "bond0.88"
        },
        "vllm_logs": {
          "weight_loading_time": 8.89,
          "model_loading_gb": 12.5523,
          "model_loading_time": 9.003535,
          "memory_profiling_time": 0.7,
          "total_gpu_memory_gb": 31.36,
          "gpu_memory_utilization": 0.9,
          "model_weights_gb": 12.55,
          "kv_cache_gb": 15.12,
          "cuda_blocks": 1935,
          "cpu_blocks": 512,
          "max_concurrency": 7.56,
          "graph_capturing_time": 8.0,
          "graph_capturing_memory_gb": 0.29,
          "init_engine_time": 10.21,
          "raw_logs": [
            "This model supports multiple tasks: {'embed', 'score', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.",
            "Initializing a V0 LLM engine (v0.7.4.dev254+ged6ea0657.d20260102) with config: model='/acpl-ssd20/1218/A', speculative_config=None, tokenizer='/acpl-ssd20/1218/A', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/acpl-ssd20/1218/A, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, ",
            "Using Flash Attention backend.",
            "rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0",
            "Starting to load model /acpl-ssd20/1218/A...",
            "Loading weights took 8.89 seconds",
            "Model loading took 12.5523 GB and 9.003535 seconds",
            "Memory profiling takes 0.70 seconds\nthe current vLLM instance can use total_gpu_memory (31.36GiB) x gpu_memory_utilization (0.90) = 28.22GiB\nmodel weights take 12.55GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.44GiB; the rest of the memory reserved for KV Cache is 15.12GiB.",
            "# cuda blocks: 1935, # CPU blocks: 512",
            "Maximum concurrency for 4096 tokens per request: 7.56x",
            "Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.",
            "Graph capturing finished in 8 secs, took 0.29 GiB",
            "init engine (profile, create kv cache, warmup model) took 10.21 seconds"
          ]
        },
        "activation_progress": "75%",
        "layers_pruned": 8
      },
      "baseline": {
        "model_type": "baseline",
        "model_path": "/acpl-ssd20/Llama-2-7b",
        "timestamp": "2026-01-09T19:03:54.738052",
        "cold_start_time": 29.526405572891235,
        "ttft": 0.021445751190185547,
        "output_throughput": 207.8436716585506,
        "gpu_memory_gb": 27.89668607711792,
        "total_layers": 32,
        "active_layers": 32,
        "network_metrics": {
          "error": true,
          "error_message": "Interface 'bond0.88' not found. Available interfaces: ['lo', 'eth0']",
          "interface": "bond0.88"
        },
        "vllm_logs": {
          "weight_loading_time": 15.49,
          "model_loading_gb": 12.5513,
          "model_loading_time": 15.518019,
          "memory_profiling_time": 0.44,
          "total_gpu_memory_gb": 31.36,
          "gpu_memory_utilization": 0.9,
          "model_weights_gb": 12.55,
          "kv_cache_gb": 15.32,
          "cuda_blocks": 1961,
          "cpu_blocks": 512,
          "max_concurrency": 7.66,
          "graph_capturing_time": 8.0,
          "graph_capturing_memory_gb": 0.06,
          "init_engine_time": 8.44,
          "raw_logs": [
            "This model supports multiple tasks: {'embed', 'score', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.",
            "Initializing a V0 LLM engine (v0.7.4.dev254+ged6ea0657.d20260102) with config: model='/acpl-ssd20/Llama-2-7b', speculative_config=None, tokenizer='/acpl-ssd20/Llama-2-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/acpl-ssd20/Llama-2-7b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, ",
            "Starting to load model /acpl-ssd20/Llama-2-7b...",
            "Loading weights took 15.49 seconds",
            "Model loading took 12.5513 GB and 15.518019 seconds",
            "Memory profiling takes 0.44 seconds\nthe current vLLM instance can use total_gpu_memory (31.36GiB) x gpu_memory_utilization (0.90) = 28.22GiB\nmodel weights take 12.55GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.35GiB; the rest of the memory reserved for KV Cache is 15.32GiB.",
            "# cuda blocks: 1961, # CPU blocks: 512",
            "Maximum concurrency for 4096 tokens per request: 7.66x",
            "Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.",
            "Graph capturing finished in 8 secs, took 0.06 GiB",
            "init engine (profile, create kv cache, warmup model) took 8.44 seconds"
          ]
        }
      },
      "experiment_id": "69b49ec1-5759-4e0c-9ca8-e8063921412c",
      "hostname": "e3a8e045298d"
    },
    {
      "mode": "reversed",
      "timestamp": "2026-01-09T19:13:47.254082",
      "progressive": {
        "model_type": "progressive",
        "model_path": "/acpl-ssd20/1218/A",
        "timestamp": "2026-01-09T19:14:08.272257",
        "cold_start_time": 20.50055480003357,
        "ttft": 0.031855106353759766,
        "output_throughput": 206.0016089946534,
        "gpu_memory_gb": 27.693454265594482,
        "total_layers": 32,
        "active_layers": 24,
        "network_metrics": {
          "error": true,
          "error_message": "Interface 'bond0.88' not found. Available interfaces: ['lo', 'eth0']",
          "interface": "bond0.88"
        },
        "vllm_logs": {
          "weight_loading_time": 9.24,
          "model_loading_gb": 12.5523,
          "model_loading_time": 9.36043,
          "memory_profiling_time": 0.71,
          "total_gpu_memory_gb": 31.36,
          "gpu_memory_utilization": 0.9,
          "model_weights_gb": 12.55,
          "kv_cache_gb": 15.12,
          "cuda_blocks": 1935,
          "cpu_blocks": 512,
          "max_concurrency": 7.56,
          "graph_capturing_time": 8.0,
          "graph_capturing_memory_gb": 0.29,
          "init_engine_time": 10.38,
          "raw_logs": [
            "This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.",
            "Initializing a V0 LLM engine (v0.7.4.dev254+ged6ea0657.d20260102) with config: model='/acpl-ssd20/1218/A', speculative_config=None, tokenizer='/acpl-ssd20/1218/A', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/acpl-ssd20/1218/A, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, ",
            "Using Flash Attention backend.",
            "rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0",
            "Starting to load model /acpl-ssd20/1218/A...",
            "Loading weights took 9.24 seconds",
            "Model loading took 12.5523 GB and 9.360430 seconds",
            "Memory profiling takes 0.71 seconds\nthe current vLLM instance can use total_gpu_memory (31.36GiB) x gpu_memory_utilization (0.90) = 28.22GiB\nmodel weights take 12.55GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.44GiB; the rest of the memory reserved for KV Cache is 15.12GiB.",
            "# cuda blocks: 1935, # CPU blocks: 512",
            "Maximum concurrency for 4096 tokens per request: 7.56x",
            "Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.",
            "Graph capturing finished in 8 secs, took 0.29 GiB",
            "init engine (profile, create kv cache, warmup model) took 10.38 seconds"
          ]
        },
        "activation_progress": "75%",
        "layers_pruned": 8
      },
      "baseline": {
        "model_type": "baseline",
        "model_path": "/acpl-ssd20/Llama-2-7b",
        "timestamp": "2026-01-09T19:14:40.258725",
        "cold_start_time": 29.43816590309143,
        "ttft": 0.021852970123291016,
        "output_throughput": 207.7079107505071,
        "gpu_memory_gb": 27.89668607711792,
        "total_layers": 32,
        "active_layers": 32,
        "network_metrics": {
          "error": true,
          "error_message": "Interface 'bond0.88' not found. Available interfaces: ['lo', 'eth0']",
          "interface": "bond0.88"
        },
        "vllm_logs": {
          "weight_loading_time": 15.39,
          "model_loading_gb": 12.5513,
          "model_loading_time": 15.418741,
          "memory_profiling_time": 0.44,
          "total_gpu_memory_gb": 31.36,
          "gpu_memory_utilization": 0.9,
          "model_weights_gb": 12.55,
          "kv_cache_gb": 15.32,
          "cuda_blocks": 1961,
          "cpu_blocks": 512,
          "max_concurrency": 7.66,
          "graph_capturing_time": 8.0,
          "graph_capturing_memory_gb": 0.06,
          "init_engine_time": 8.44,
          "raw_logs": [
            "This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.",
            "Initializing a V0 LLM engine (v0.7.4.dev254+ged6ea0657.d20260102) with config: model='/acpl-ssd20/Llama-2-7b', speculative_config=None, tokenizer='/acpl-ssd20/Llama-2-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/acpl-ssd20/Llama-2-7b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, ",
            "Starting to load model /acpl-ssd20/Llama-2-7b...",
            "Loading weights took 15.39 seconds",
            "Model loading took 12.5513 GB and 15.418741 seconds",
            "Memory profiling takes 0.44 seconds\nthe current vLLM instance can use total_gpu_memory (31.36GiB) x gpu_memory_utilization (0.90) = 28.22GiB\nmodel weights take 12.55GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.35GiB; the rest of the memory reserved for KV Cache is 15.32GiB.",
            "# cuda blocks: 1961, # CPU blocks: 512",
            "Maximum concurrency for 4096 tokens per request: 7.66x",
            "Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.",
            "Graph capturing finished in 8 secs, took 0.06 GiB",
            "init engine (profile, create kv cache, warmup model) took 8.44 seconds"
          ]
        }
      },
      "experiment_id": "7bf00b44-dc24-4578-9394-952b618ab71d",
      "hostname": "e3a8e045298d"
    }
  ]
}